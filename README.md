# RAG-System-Q-A-
PDF Q&A System â€“ RAG with LangChain, LLaMA3, and Flask  
This project is a Retrieval-Augmented Generation (RAG) application that lets you upload a PDF, ask questions about it, and get accurate AI-generated answers â€” all with the context pulled directly from the document.

Chroma DB and the rest of files in drive: https://drive.google.com/drive/u/3/folders/1P9sRY-Y6E9sGleqkoTPR1NeRy4sGdux7  


Itâ€™s perfect for research papers, reports, books, technical manuals, and any long-form document where searching manually would be slow.

ðŸš€ What It Can Do
Upload and process PDFs (supports large documents)

Automatically clean & split the document into manageable chunks

Generate embeddings for each chunk using FastEmbed

Store and search document embeddings in ChromaDB (persistent vector store)

Retrieve the most relevant context for your question

Answer questions using LLaMA3 via Ollama (local inference, no API cost)

Simple Flask Web Interface so you donâ€™t need to run Python commands each time

ðŸ›  Tech Stack
LangChain â€“ For document loading, chunking, retrieval pipeline

FastEmbed â€“ For lightweight and fast embeddings

ChromaDB â€“ For vector storage & similarity search

Ollama â€“ To run LLaMA3 locally

Flask â€“ To create the user-friendly web app


